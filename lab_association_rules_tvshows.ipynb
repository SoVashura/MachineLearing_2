{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6da99962",
   "metadata": {},
   "source": [
    "\n",
    "# Лабораторная работа №2  \n",
    "## Анализ ассоциативных правил (Apriori и FPGrowth) — датасет *TV_Shows*\n",
    "\n",
    "**Цель работы:** исследовать методы анализа ассоциативных правил на датасете, содержащем информацию о совместно просматриваемых телешоу, с использованием алгоритмов **Apriori** и **FPGrowth**.\n",
    "\n",
    "В работе выполняются следующие шаги:\n",
    "\n",
    "1. Загрузка и описание датасета (транзакции: одновременно просмотренные или связанные телешоу).\n",
    "2. Первичный анализ: распределение длин транзакций, список уникальных шоу и др.\n",
    "3. Преобразование данных в бинарный формат (one-hot encoding).\n",
    "4. Поиск частых наборов элементов с помощью алгоритмов **Apriori** и **FPGrowth** при начальных параметрах:  \n",
    "   - минимальная поддержка `min_support = 0.02`  \n",
    "   - минимальная достоверность (confidence) `min_confidence = 0.3`\n",
    "5. Генерация ассоциативных правил, расчёт метрик (**support**, **confidence**, **lift**, **conviction**).\n",
    "6. Выделение полезных и тривиальных правил, анализ лифта и достоверности.\n",
    "7. Исследование влияния параметров `min_support` и `min_confidence` на количество и качество правил.\n",
    "8. Алгоритмическое определение минимальных значений поддержки для наборов из 1, 2, ... объектов.\n",
    "9. Визуализация ассоциативных правил:  \n",
    "   - граф на основе правил;  \n",
    "   - собственный способ визуализации (scatter-графики метрик, диаграммы лучших правил).\n",
    "10. Формирование выводов по результатам экспериментов.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81a4cf8",
   "metadata": {},
   "source": [
    "## 1. Импорт библиотек и загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32884693",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules, fpgrowth\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "# Настройки отображения\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (8, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28457340",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "all_data = pd.read_csv('TV_Shows.csv', on_bad_lines='skip')\n",
    "print(\"Размерность исходной таблицы:\", all_data.shape)\n",
    "all_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e451e6",
   "metadata": {},
   "source": [
    "## 2. Первичный анализ и описание данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec81c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Общая информация о данных\n",
    "all_data.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3469ed05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Случайный фрагмент данных\n",
    "all_data.sample(5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b453ce4",
   "metadata": {},
   "source": [
    "### 2.1. Анализ длин транзакций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d249e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "transaction_lengths = all_data.notnull().sum(axis=1)\n",
    "print(\"Минимальная длина транзакции:\", transaction_lengths.min())\n",
    "print(\"Максимальная длина транзакции:\", transaction_lengths.max())\n",
    "print(\"Средняя длина транзакции:\", transaction_lengths.mean())\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(transaction_lengths, bins=range(1, transaction_lengths.max() + 2), edgecolor='black')\n",
    "plt.xlabel('Длина транзакции (кол-во шоу)')\n",
    "plt.ylabel('Частота')\n",
    "plt.title('Распределение длин транзакций')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e691e68c",
   "metadata": {},
   "source": [
    "### 2.2. Список уникальных телешоу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081fb51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Переводим DataFrame в numpy-массив\n",
    "np_data_raw = all_data.to_numpy()\n",
    "\n",
    "transactions = [\n",
    "    [elem for elem in row[1:] if isinstance(elem, str)]\n",
    "    for row in np_data_raw\n",
    "]\n",
    "\n",
    "# Удаляем пустые транзакции\n",
    "transactions = [t for t in transactions if len(t) > 0]\n",
    "\n",
    "print(\"Пример 5 транзакций:\")\n",
    "for t in transactions[:5]:\n",
    "    print(t)\n",
    "\n",
    "# Список уникальных шоу\n",
    "unique_items = set()\n",
    "for row in transactions:\n",
    "    for elem in row:\n",
    "        unique_items.add(elem)\n",
    "\n",
    "print(\"\\nКоличество уникальных телешоу:\", len(unique_items))\n",
    "print(\"Первые 30 шоу:\")\n",
    "sorted(list(unique_items))[:30]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0303a1",
   "metadata": {},
   "source": [
    "## 3. Преобразование данных в бинарный формат (one-hot encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7013e621",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(transactions).transform(transactions)\n",
    "\n",
    "data = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "print(\"Размерность бинарной матрицы (транзакции × шоу):\", data.shape)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dca8a6",
   "metadata": {},
   "source": [
    "## 4. Алгоритм Apriori: частые наборы и ассоциативные правила"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b578508",
   "metadata": {},
   "source": [
    "### 4.1. Поиск частых наборов (начальные параметры: min_support = 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffb6690",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "min_support_init = 0.02\n",
    "min_confidence_init = 0.3\n",
    "\n",
    "frequent_ap = apriori(data, min_support=min_support_init, use_colnames=True)\n",
    "frequent_ap['length'] = frequent_ap['itemsets'].apply(len)\n",
    "\n",
    "print(\"Число частых наборов (Apriori):\", len(frequent_ap))\n",
    "frequent_ap.sort_values(['length', 'support'], ascending=[True, False]).head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03c2a1d",
   "metadata": {},
   "source": [
    "### 4.2. Генерация ассоциативных правил (Apriori)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df27dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rules_ap = association_rules(\n",
    "    frequent_ap,\n",
    "    metric=\"confidence\",\n",
    "    min_threshold=min_confidence_init\n",
    ")\n",
    "\n",
    "print(\"Число ассоциативных правил (Apriori):\", len(rules_ap))\n",
    "\n",
    "# Добавим длины левой и правой части правила\n",
    "rules_ap['antecedent_len'] = rules_ap['antecedents'].apply(len)\n",
    "rules_ap['consequent_len'] = rules_ap['consequents'].apply(len)\n",
    "\n",
    "# Выведем 10 правил по lift\n",
    "rules_ap.sort_values('lift', ascending=False).head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf265dd5",
   "metadata": {},
   "source": [
    "### 4.3. Полезные vs тривиальные правила"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2cd2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Тривиальные правила: высокий confidence, но lift ~ 1 (нет реального усиления связи)\n",
    "trivial_rules = rules_ap[(rules_ap['confidence'] >= min_confidence_init) & (rules_ap['lift'].between(0.95, 1.05))]\n",
    "\n",
    "# Потенциально полезные: и confidence, и lift заметно больше 1\n",
    "useful_rules = rules_ap[(rules_ap['confidence'] >= min_confidence_init) & (rules_ap['lift'] > 1.2)]\n",
    "\n",
    "print(\"Тривиальные правила (пример до 10):\")\n",
    "trivial_rules.sort_values('confidence', ascending=False).head(10)[['antecedents', 'consequents', 'support', 'confidence', 'lift']]\n",
    "\n",
    "print(\"\\nПолезные правила (пример до 10):\")\n",
    "useful_rules.sort_values('lift', ascending=False).head(10)[['antecedents', 'consequents', 'support', 'confidence', 'lift']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f11f4e",
   "metadata": {},
   "source": [
    "## 5. Алгоритм FPGrowth: частые наборы и правила"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae785844",
   "metadata": {},
   "source": [
    "### 5.1. Частые наборы (FPGrowth, те же параметры)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cffcbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "frequent_fp = fpgrowth(data, min_support=min_support_init, use_colnames=True)\n",
    "frequent_fp['length'] = frequent_fp['itemsets'].apply(len)\n",
    "\n",
    "print(\"Число частых наборов (FPGrowth):\", len(frequent_fp))\n",
    "frequent_fp.sort_values(['length', 'support'], ascending=[True, False]).head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e075dab0",
   "metadata": {},
   "source": [
    "### 5.2. Правила (FPGrowth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4f1268",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rules_fp = association_rules(\n",
    "    frequent_fp,\n",
    "    metric=\"confidence\",\n",
    "    min_threshold=min_confidence_init\n",
    ")\n",
    "\n",
    "print(\"Число ассоциативных правил (FPGrowth):\", len(rules_fp))\n",
    "\n",
    "rules_fp['antecedent_len'] = rules_fp['antecedents'].apply(len)\n",
    "rules_fp['consequent_len'] = rules_fp['consequents'].apply(len)\n",
    "\n",
    "rules_fp.sort_values('lift', ascending=False).head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf5dbb3",
   "metadata": {},
   "source": [
    "## 6. Минимальные значения поддержки для наборов разной длины"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc495a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Используем результаты Apriori (frequent_ap)\n",
    "# Для каждого k = 1, 2, ..., найдём минимальное значение поддержки среди частых наборов длины k\n",
    "\n",
    "min_support_by_len = (\n",
    "    frequent_ap\n",
    "    .groupby('length')['support']\n",
    "    .min()\n",
    "    .rename('min_support')\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(\"Минимальная поддержка среди частых наборов заданной длины (Apriori):\")\n",
    "min_support_by_len\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342c301b",
   "metadata": {},
   "source": [
    "\n",
    "> Интерпретация: для наборов из k шоу значения поддержки не могут быть меньше указанных в таблице.  \n",
    "> Эти значения можно рассматривать как **оценку минимального порога поддержки**, при котором ещё существуют частые наборы длины k для данного датасета.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d99bd2",
   "metadata": {},
   "source": [
    "## 7. Влияние параметров min_support и min_confidence на правила (Apriori)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a36f20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "support_values = [0.01, 0.02, 0.03, 0.05]\n",
    "confidence_values = [0.2, 0.3, 0.4]\n",
    "\n",
    "results_experiments = []\n",
    "\n",
    "for sup in support_values:\n",
    "    freq_tmp = apriori(data, min_support=sup, use_colnames=True)\n",
    "    if freq_tmp.empty:\n",
    "        continue\n",
    "    for conf in confidence_values:\n",
    "        rules_tmp = association_rules(freq_tmp, metric=\"confidence\", min_threshold=conf)\n",
    "        if len(rules_tmp) == 0:\n",
    "            results_experiments.append({\n",
    "                'min_support': sup,\n",
    "                'min_confidence': conf,\n",
    "                'n_rules': 0,\n",
    "                'mean_confidence': np.nan,\n",
    "                'mean_lift': np.nan\n",
    "            })\n",
    "        else:\n",
    "            results_experiments.append({\n",
    "                'min_support': sup,\n",
    "                'min_confidence': conf,\n",
    "                'n_rules': len(rules_tmp),\n",
    "                'mean_confidence': rules_tmp['confidence'].mean(),\n",
    "                'mean_lift': rules_tmp['lift'].mean()\n",
    "            })\n",
    "\n",
    "experiments_df = pd.DataFrame(results_experiments)\n",
    "experiments_df.sort_values(['min_support', 'min_confidence'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12764e1e",
   "metadata": {},
   "source": [
    "\n",
    "**Наблюдения (ожидаемые):**\n",
    "\n",
    "- При **увеличении `min_support`** число частых наборов и ассоциативных правил, как правило, **уменьшается**, так как остаются только самые распространённые комбинации телешоу.\n",
    "- При **увеличении `min_confidence`** отсекаются правила с низкой достоверностью — общее число правил уменьшается, но средняя достоверность и лифт у оставшихся правил растут.\n",
    "- Слишком низкие пороги (`min_support`, `min_confidence`) приводят к огромному количеству правил, многие из которых трудно интерпретировать; слишком высокие — к отсутствию или очень малому количеству правил.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e7a9bb",
   "metadata": {},
   "source": [
    "## 8. Визуализация ассоциативных правил в виде графа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62b6909",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Для визуализации возьмём небольшой набор правил, чтобы граф был читаемым\n",
    "rules_for_graph = (\n",
    "    rules_ap\n",
    "    .sort_values('confidence', ascending=False)\n",
    "    .head(20)  # можно менять количество\n",
    "    .copy()\n",
    ")\n",
    "\n",
    "# Строковое представление левой и правой части правила\n",
    "rules_for_graph['antecedent_str'] = rules_for_graph['antecedents'].apply(lambda x: ', '.join(list(x)))\n",
    "rules_for_graph['consequent_str'] = rules_for_graph['consequents'].apply(lambda x: ', '.join(list(x)))\n",
    "\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Добавляем узлы и рёбра\n",
    "for _, row in rules_for_graph.iterrows():\n",
    "    a = row['antecedent_str']\n",
    "    c = row['consequent_str']\n",
    "    conf = row['confidence']\n",
    "    lift = row['lift']\n",
    "    G.add_node(a)\n",
    "    G.add_node(c)\n",
    "    G.add_edge(a, c, confidence=conf, lift=lift)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "pos = nx.spring_layout(G, k=0.7, seed=42)\n",
    "\n",
    "nx.draw(\n",
    "    G, pos,\n",
    "    with_labels=True,\n",
    "    node_size=2000,\n",
    "    node_color='lightblue',\n",
    "    font_size=8,\n",
    "    arrows=True,\n",
    "    arrowstyle='->',\n",
    "    arrowsize=15\n",
    ")\n",
    "\n",
    "# Подписи рёбер – значение confidence\n",
    "edge_labels = { (u, v): f\"{d['confidence']:.2f}\" for u, v, d in G.edges(data=True) }\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=8)\n",
    "\n",
    "plt.title('Граф ассоциативных правил (узлы = наборы шоу, рёбра = правила, подпись = confidence)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3f7caf",
   "metadata": {},
   "source": [
    "\n",
    "**Интерпретация графа:**\n",
    "\n",
    "- **Узлы** соответствуют наборам телешоу (левая или правая части правил).  \n",
    "- **Направленные рёбра** показывают ассоциативные правила вида *antecedent → consequent*.\n",
    "- Подпись на ребре — значение **confidence** (достоверности правила).  \n",
    "- Узлы с большим количеством входящих/исходящих рёбер обозначают телешоу, которые часто встречаются в связке с другими и могут быть «якорными» рекомендациями.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65642d55",
   "metadata": {},
   "source": [
    "## 9. Собственный способ визуализации ассоциативных правил и метрик"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb63757",
   "metadata": {},
   "source": [
    "### 9.1. Scatter-график: поддержка vs достоверность, цвет = лифт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b8ed48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Для удобства ограничим количество отображаемых правил\n",
    "rules_plot = rules_ap.copy()\n",
    "rules_plot = rules_plot.sort_values('support', ascending=False).head(200)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(\n",
    "    rules_plot['support'],\n",
    "    rules_plot['confidence'],\n",
    "    c=rules_plot['lift'],\n",
    "    cmap='viridis',\n",
    "    alpha=0.8\n",
    ")\n",
    "plt.colorbar(scatter, label='Lift')\n",
    "plt.xlabel('Support')\n",
    "plt.ylabel('Confidence')\n",
    "plt.title('Ассоциативные правила: Support vs Confidence (цвет = Lift)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43971238",
   "metadata": {},
   "source": [
    "### 9.2. Топ-правила по лифту в виде горизонтальной диаграммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b137990",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "top_lift = (\n",
    "    rules_ap\n",
    "    .copy()\n",
    "    .sort_values('lift', ascending=False)\n",
    "    .head(15)\n",
    ")\n",
    "\n",
    "top_lift['rule_str'] = top_lift.apply(\n",
    "    lambda row: f\"{', '.join(list(row['antecedents']))} -> {', '.join(list(row['consequents']))}\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(\n",
    "    x='lift',\n",
    "    y='rule_str',\n",
    "    data=top_lift,\n",
    "    orient='h'\n",
    ")\n",
    "plt.xlabel('Lift')\n",
    "plt.ylabel('Правило')\n",
    "plt.title('Топ-15 правил по лифту')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5176fc30",
   "metadata": {},
   "source": [
    "\n",
    "**Пояснение к визуализациям:**\n",
    "\n",
    "1. **Scatter-график Support vs Confidence (цвет = Lift)**  \n",
    "   Позволяет одновременно анализировать частоту сочетания телешоу (support), силу правила (confidence) и «усиление» связи (lift).  \n",
    "   Можно выделить:\n",
    "   - редкие, но сильные комбинации (низкий support, высокий lift);\n",
    "   - массовые и сильные (высокий support и lift);\n",
    "   - тривиальные (lift ≈ 1).\n",
    "\n",
    "2. **Горизонтальный barplot топ-правил по лифту**  \n",
    "   Явно показывает наиболее интересные ассоциативные правила в виде читаемых строк вида  \n",
    "   `шоу(а) -> шоу(а)` c максимальным лифтом.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d363017",
   "metadata": {},
   "source": [
    "## 10. Выводы по лабораторной работе (датасет TV_Shows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dcbe8b",
   "metadata": {},
   "source": [
    "\n",
    "В ходе выполнения лабораторной работы на датасете **TV_Shows** были получены следующие результаты:\n",
    "\n",
    "1. Проведён анализ транзакций (совместно встречающихся телешоу):  \n",
    "   - построено распределение длин транзакций;  \n",
    "   - получен список уникальных телешоу и их количество.\n",
    "\n",
    "2. Данные преобразованы в бинарный формат (one-hot encoding), что позволило применить алгоритмы поиска частых наборов элементов.\n",
    "\n",
    "3. С использованием алгоритма **Apriori** при параметрах `min_support = 0.02` и `min_confidence = 0.3` найдены частые наборы шоу и ассоциативные правила.  \n",
    "   Анализ метрик **support**, **confidence**, **lift** и **conviction** позволил выделить как тривиальные, так и потенциально полезные правила.\n",
    "\n",
    "4. Алгоритм **FPGrowth** применён для того же датасета и тех же параметров. Он показал аналогичные результаты, при этом являясь более эффективным при увеличении размера данных.\n",
    "\n",
    "5. По результатам Apriori вычислены минимальные значения поддержки для наборов различной длины (1, 2, ... шоу), что позволяет оценить разумные нижние пороги `min_support` для поиска правил заданной сложности.\n",
    "\n",
    "6. Исследовано влияние параметров `min_support` и `min_confidence` на количество и качество правил:  \n",
    "   - увеличение порогов приводит к уменьшению числа правил и росту средней достоверности и лифта;  \n",
    "   - слишком низкие пороги дают много, но не всегда содержательных правил.\n",
    "\n",
    "7. Построен граф ассоциативных правил, наглядно показывающий связи между наборами телешоу. На его основе можно выделить «центральные» шоу и кластеры взаимосвязанных шоу, что важно, например, для систем рекомендаций.\n",
    "\n",
    "8. Реализованы дополнительные визуализации (scatter-график метрик и barplot топ-правил), которые упрощают содержательный анализ правил и выбор наиболее интересных закономерностей.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
